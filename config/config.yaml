# AI Search Engine Configuration

# LLM Providers Configuration
llm:
  # OpenAI Configuration
  openai:
    enabled: false  # Enable if OpenAI API key is available
    api_key: ${OPENAI_API_KEY}
    model: gpt-3.5-turbo
    temperature: 0.7
    max_tokens: 2000
    # Custom API endpoint (optional, defaults to https://api.openai.com/v1)
    base_url: https://api.openai.com/v1
    provider_name: "OpenAI"

  # Aliyun DashScope Configuration (Qwen models via OpenAI-compatible API)
  # Documentation: https://help.aliyun.com/zh/dashscope/developer-reference/
  dashscope:
    enabled: true
    api_key: ${DASHSCOPE_API_KEY}
    model: qwen3-max
    temperature: 0.7
    max_tokens: 20000
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    provider_name: "Aliyun DashScope"

  # Ollama Local Model Configuration
  ollama:
    enabled: false
    base_url: http://localhost:11434
    model: llama2
    temperature: 0.7

  # OpenAI-compatible APIs (same format as OpenAI but different endpoints)
  # These use the OpenAI client with custom base URLs
  openai_compatible:
    # DeepSeek - https://deepseek.com
    deepseek:
      enabled: false
      api_key: ${DEEPSEEK_API_KEY}
      model: deepseek-chat
      base_url: https://api.deepseek.com
      temperature: 0.7
      max_tokens: 2000
      provider_name: "DeepSeek"

    # Claude (via OpenAI-compatible endpoint if available)
    # Note: Claude API uses different format, this is for reference
    # claude:
    #   enabled: false
    #   api_key: ${ANTHROPIC_API_KEY}
    #   model: claude-3-sonnet
    #   base_url: https://api.anthropic.com/v1
    #   provider_name: "Claude"

    # Local OpenAI-compatible server (e.g., LM Studio, vLLM)
    local_compatible:
      enabled: false
      api_key: "local-key"
      model: llama-2
      base_url: http://localhost:8000/v1
      temperature: 0.7
      max_tokens: 2000
      provider_name: "LocalOpenAI"

    # Other OpenAI-compatible providers
    # Example: Together AI, Replicate, etc.
    # together_ai:
    #   enabled: false
    #   api_key: ${TOGETHER_API_KEY}
    #   model: meta-llama/Llama-2-7b-hf
    #   base_url: https://api.together.xyz/v1
    #   provider_name: "TogetherAI"

# Search Configuration
search:
  provider: serpapi  # Options: serpapi, google_search, bing
  serpapi_key: ${SERPAPI_API_KEY}
  results_per_query: 5
  timeout: 10

# Web Scraping Configuration
scraper:
  timeout: 10
  max_workers: 5
  user_agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"

# Code Execution Configuration
code_execution:
  # Execution timeout in seconds
  timeout: 30

  # Maximum lines of output to display (prevents memory issues from infinite loops)
  max_output_lines: 1000

  # ================================================================
  # SECURITY CONFIGURATION (New in 2025-11-04)
  # ================================================================

  # Security level for AST-based code validation
  # Options:
  #   - strict: Only basic Python (no imports, math operations only)
  #   - moderate: + safe libraries (math, datetime, json, itertools) [RECOMMENDED]
  #   - permissive: + data science libraries (numpy, pandas, scipy, matplotlib)
  # Default: moderate
  security_level: moderate

  # Enable Docker sandbox for isolated code execution
  # - true: Run code in Docker containers with resource limits (RECOMMENDED)
  # - false: Fallback to subprocess execution (less secure)
  # Default: true
  # Requirements: Docker installed and running
  enable_docker: true

  # Enable AST-based code validation (Layer 1 security)
  # - true: Validate code before execution [RECOMMENDED]
  # - false: Skip validation (UNSAFE - only for testing)
  # Default: true
  enable_validation: true

  # Docker memory limit (only used when enable_docker=true)
  # Examples: "128m", "256m", "512m", "1g"
  # Default: "256m"
  memory_limit: "256m"

  # Docker CPU limit (CPU cores)
  # Examples: 0.5, 1.0, 2.0
  # Default: 1.0
  cpu_limit: 1.0

  # Enable network access in sandbox
  # - false: No network access (RECOMMENDED for security)
  # - true: Allow network (required for packages like requests)
  # Default: false
  enable_network: false

  # Python version for Docker sandbox
  # Options: "3.11", "3.10", "3.9", etc.
  # Default: "3.11"
  python_version: "3.11"

  # Maximum output size in bytes (prevents memory exhaustion)
  # Default: 100000 (100KB)
  max_output_size: 100000

  # ================================================================
  # LEGACY: Allowed Imports (Deprecated)
  # ================================================================
  # NOTE: This parameter is now controlled by the 'security_level' setting above.
  # The security_level determines which modules are allowed:
  #   - strict: No imports allowed
  #   - moderate: math, statistics, random, datetime, json, etc.
  #   - permissive: moderate + numpy, pandas, scipy, matplotlib, sympy
  #
  # This list is kept for backward compatibility but is no longer actively used.
  allowed_imports:
    - numpy
    - pandas
    - scipy
    - matplotlib
    - sympy
    - math
    - statistics

# Research Agent Configuration
research:
  max_queries: 5
  top_results_per_query: 3
  summary_max_tokens: 5000

# CLI Configuration
cli:
  theme: monokai
  verbose: false
  show_sources: true

# Cache Configuration
cache:
  enabled: true
  ttl_seconds: 3600
  backend: sqlite  # Options: sqlite, redis
  db_path: ./cache/search_cache.db

# RAG Configuration
rag:
  enabled: true
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"  # Lightweight model
  # For Chinese/English: "jinaai/jina-embeddings-v2-base-zh" (requires pip install jina-embeddings-v2)
  embedding_dimension: 384
  device: "cpu"  # or "cuda" for GPU

  # Vector store settings
  persist_directory: "./data/vector_store"
  collection_name: "documents"

  # Chunking strategy
  chunking:
    strategy: "semantic"  # Options: fixed, semantic, recursive
    chunk_size: 512       # Target chunk size in characters
    chunk_overlap: 77     # 15% overlap (recommended)
    min_chunk_size: 100   # Minimum chunk size

  # Retrieval settings
  retrieval:
    top_k: 10               # Number of chunks to retrieve
    similarity_threshold: 0.7  # Minimum similarity score

  # Reranking (Phase 2)
  reranking:
    enabled: false  # Enable after installing BGE reranker
    model: "BAAI/bge-reranker-large"
    top_k: 3

# Domain-Specific Tools Configuration
domain_tools:
  # Weather API
  weather:
    enabled: true
    provider: "openweathermap"
    api_key: ${OPENWEATHERMAP_API_KEY}
    units: "metric"  # or "imperial"
    language: "zh_cn"  # Chinese

  # Finance APIs
  finance:
    enabled: true
    primary_provider: "alpha_vantage"
    alpha_vantage_key: ${ALPHA_VANTAGE_API_KEY}
    fallback_provider: "yfinance"
    cache_ttl: 300  # 5 minutes

  # Routing/Transportation
  routing:
    enabled: true
    provider: "openrouteservice"
    api_key: ${OPENROUTESERVICE_API_KEY}
    default_profile: "driving-car"  # Options: driving-car, cycling, walking

# Multimodal Support Configuration
multimodal:
  # OCR settings
  ocr:
    enabled: true
    provider: "paddleocr"
    languages: ["ch", "en"]  # Chinese and English
    use_gpu: false

  # Vision API settings
  vision:
    enabled: true
    provider: "gemini"  # Options: gemini, gpt4v, claude
    model: "gemini-2.5-pro"
    api_key: ${GOOGLE_API_KEY}
