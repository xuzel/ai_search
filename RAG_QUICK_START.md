# RAG ç³»ç»Ÿå¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸš€ 5 åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹

### æ­¥éª¤ 1: å®‰è£…æ ¸å¿ƒä¾èµ–

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# åªå®‰è£… RAG å¿…éœ€çš„åº“ (å…¶ä»–çš„å¯ä»¥åç»­å®‰è£…)
pip install llama-index==0.10.12
pip install chromadb==0.4.22
pip install sentence-transformers==2.3.1
pip install pymupdf==1.23.8
pip install python-docx==1.1.0
```

### æ­¥éª¤ 2: å‡†å¤‡æµ‹è¯•æ–‡æ¡£

```bash
# åˆ›å»ºæ–‡æ¡£ç›®å½•
mkdir -p ./data/documents

# å¤åˆ¶ä½ çš„ PDF/TXT/MD/DOCX æ–‡ä»¶åˆ°è¿™ä¸ªç›®å½•
cp /path/to/your/document.pdf ./data/documents/
```

### æ­¥éª¤ 3: è¿è¡Œç¬¬ä¸€ä¸ª RAG ç¤ºä¾‹

åˆ›å»º `examples/rag_demo.py`:

```python
"""RAG ç³»ç»Ÿæ¼”ç¤º"""

import asyncio
from src.agents import RAGAgent
from src.llm import LLMManager
from src.utils import get_config


async def main():
    # åˆå§‹åŒ–
    print("ğŸ”§ åˆå§‹åŒ– RAG ç³»ç»Ÿ...")
    config = get_config()
    llm_manager = LLMManager(config=config)
    rag_agent = RAGAgent(llm_manager=llm_manager, config=config)

    # æ£€æŸ¥å‘é‡åº“çŠ¶æ€
    stats = rag_agent.get_stats()
    print(f"\nğŸ“Š å½“å‰å‘é‡åº“: {stats['total_documents']} ä¸ªæ–‡æ¡£å—")

    # 1. æ‘„å–æ–‡æ¡£
    print("\n" + "=" * 50)
    print("æ­¥éª¤ 1: æ‘„å–æ–‡æ¡£")
    print("=" * 50)

    result = await rag_agent.ingest_document(
        file_path="./data/documents/your_document.pdf",  # ä¿®æ”¹ä¸ºä½ çš„æ–‡ä»¶
        show_progress=True,
    )

    print(f"\nâœ… æ‘„å–å®Œæˆ!")
    print(f"   - æ–‡ä»¶: {result['file_path']}")
    print(f"   - æå–ç« èŠ‚: {result['sections']}")
    print(f"   - ç”Ÿæˆå—: {result['chunks']}")

    # 2. æŸ¥è¯¢æ–‡æ¡£
    print("\n" + "=" * 50)
    print("æ­¥éª¤ 2: æŸ¥è¯¢æ–‡æ¡£")
    print("=" * 50)

    questions = [
        "è¿™ä¸ªæ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æ–‡æ¡£ä¸­æœ‰å“ªäº›å…³é”®æ¦‚å¿µï¼Ÿ",
        "èƒ½å¦æ€»ç»“æ–‡æ¡£çš„æ ¸å¿ƒè§‚ç‚¹ï¼Ÿ",
    ]

    for i, question in enumerate(questions, 1):
        print(f"\né—®é¢˜ {i}: {question}")
        print("-" * 50)

        answer_result = await rag_agent.query(
            question=question,
            show_progress=False,
        )

        print(f"ğŸ’¡ å›ç­”: {answer_result['answer']}")
        print(f"ğŸ“š å¼•ç”¨æ¥æº: {answer_result['retrieved_chunks']} ä¸ªç›¸å…³ç‰‡æ®µ")

    # 3. æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    print("\n" + "=" * 50)
    print("ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 50)

    final_stats = rag_agent.get_stats()
    print(f"å‘é‡åº“æ€»å—æ•°: {final_stats['total_documents']}")
    print(f"å­˜å‚¨ä½ç½®: {final_stats['persist_directory']}")


if __name__ == "__main__":
    asyncio.run(main())
```

### æ­¥éª¤ 4: è¿è¡Œ

```bash
python examples/rag_demo.py
```

**é¢„æœŸè¾“å‡º**:
```
ğŸ”§ åˆå§‹åŒ– RAG ç³»ç»Ÿ...
Loading embedding model: sentence-transformers/all-MiniLM-L6-v2

ğŸ“Š å½“å‰å‘é‡åº“: 0 ä¸ªæ–‡æ¡£å—

==================================================
æ­¥éª¤ 1: æ‘„å–æ–‡æ¡£
==================================================

ğŸ“„ Processing document: ./data/documents/your_document.pdf
âœ… Extracted 10 sections
ğŸ”ª Chunking documents...
âœ… Created 45 chunks
ğŸ’¾ Adding to vector store...
Generating embeddings for 45 documents...
âœ… Ingested 45 chunks

âœ… æ‘„å–å®Œæˆ!
   - æ–‡ä»¶: ./data/documents/your_document.pdf
   - æå–ç« èŠ‚: 10
   - ç”Ÿæˆå—: 45

==================================================
æ­¥éª¤ 2: æŸ¥è¯¢æ–‡æ¡£
==================================================

é—®é¢˜ 1: è¿™ä¸ªæ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ
--------------------------------------------------
ğŸ” Searching documents for: è¿™ä¸ªæ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ
âœ… Found 10 relevant chunks
âœ… 5 chunks above threshold (0.7)
ğŸ¤– Generating answer...

ğŸ’¡ å›ç­”: è¿™ä¸ªæ–‡æ¡£ä¸»è¦ä»‹ç»äº†...
ğŸ“š å¼•ç”¨æ¥æº: 5 ä¸ªç›¸å…³ç‰‡æ®µ
```

---

## ğŸ“– å¸¸è§ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: æ‰¹é‡æ‘„å–ç›®å½•

```python
# æ‘„å–æ•´ä¸ªæ–‡æ¡£æ–‡ä»¶å¤¹
result = await rag_agent.ingest_directory(
    directory_path="./data/documents",
    recursive=True,  # åŒ…å«å­ç›®å½•
    show_progress=True,
)

print(f"æ‘„å–äº† {result['chunks']} ä¸ªå—")
```

### åœºæ™¯ 2: è°ƒæ•´æ£€ç´¢å‚æ•°

```python
# æ£€ç´¢æ›´å¤šå€™é€‰ï¼Œé™ä½é˜ˆå€¼
answer = await rag_agent.query(
    question="ä½ çš„é—®é¢˜",
    top_k=20,  # æ£€ç´¢ 20 ä¸ªå€™é€‰ï¼ˆé»˜è®¤ 10ï¼‰
    show_progress=True,
)
```

### åœºæ™¯ 3: æ¸…ç©ºå‘é‡åº“

```python
# æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£ï¼ˆé‡æ–°å¼€å§‹ï¼‰
rag_agent.clear_documents()
print("å‘é‡åº“å·²æ¸…ç©º")
```

### åœºæ™¯ 4: æŸ¥çœ‹è¯¦ç»†æ¥æº

```python
answer = await rag_agent.query("ä½ çš„é—®é¢˜")

# æŸ¥çœ‹æ‰€æœ‰æ¥æº
for i, source in enumerate(answer['sources'], 1):
    print(f"\næ¥æº {i}:")
    print(f"  ç›¸ä¼¼åº¦: {source['score']:.2f}")
    print(f"  å†…å®¹: {source['text']}")
    print(f"  å…ƒæ•°æ®: {source['metadata']}")
```

---

## âš™ï¸ é…ç½®ä¼˜åŒ–

### ä¼˜åŒ– 1: ä½¿ç”¨æ›´å¥½çš„åµŒå…¥æ¨¡å‹ï¼ˆä¸­è‹±åŒè¯­ï¼‰

ç¼–è¾‘ `config/config.yaml`:

```yaml
rag:
  # å‡çº§åˆ° Jina AI v2 (æ”¯æŒä¸­è‹±æ–‡ï¼Œ8K ä¸Šä¸‹æ–‡)
  embedding_model: "jinaai/jina-embeddings-v2-base-zh"
  embedding_dimension: 768  # æ›´æ–°ç»´åº¦
```

éœ€è¦å®‰è£…:
```bash
pip install jina-embeddings-v2
```

**æ•ˆæœ**: ä¸­è‹±æ–‡æ··åˆæ–‡æ¡£æ£€ç´¢å‡†ç¡®ç‡æå‡ 15-20%

### ä¼˜åŒ– 2: è°ƒæ•´åˆ†å—ç­–ç•¥

```yaml
rag:
  chunking:
    strategy: "recursive"  # æ›´æ™ºèƒ½çš„åˆ†å—
    chunk_size: 1024      # æ›´å¤§çš„å—ï¼ˆé€‚åˆé•¿æ–‡æ¡£ï¼‰
    chunk_overlap: 154    # 15% é‡å 
```

### ä¼˜åŒ– 3: é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå¬å›æ›´å¤šï¼‰

```yaml
rag:
  retrieval:
    top_k: 15                   # æ£€ç´¢æ›´å¤šå€™é€‰
    similarity_threshold: 0.5   # é™ä½é˜ˆå€¼
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: å®‰è£… chromadb å¤±è´¥

```bash
# macOS
brew install cmake

# Ubuntu
sudo apt-get install cmake

# ç„¶åé‡æ–°å®‰è£…
pip install chromadb
```

### Q2: å‘é‡æ¨¡å‹ä¸‹è½½æ…¢

```bash
# è®¾ç½® HuggingFace é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# æˆ–è€…æ‰‹åŠ¨ä¸‹è½½åæŒ‡å®šæœ¬åœ°è·¯å¾„
embedding_model: "/path/to/local/model"
```

### Q3: æŸ¥è¯¢è¿”å›"No relevant information"

**åŸå› **: ç›¸ä¼¼åº¦é˜ˆå€¼å¤ªé«˜

**è§£å†³**:
```python
# ä¸´æ—¶é™ä½é˜ˆå€¼
rag_agent.similarity_threshold = 0.5

# æˆ–åœ¨é…ç½®æ–‡ä»¶ä¿®æ”¹
```

### Q4: å†…å­˜å ç”¨è¿‡å¤§

**åŸå› **: åµŒå…¥æ¨¡å‹å ç”¨å†…å­˜

**è§£å†³**:
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹: `all-MiniLM-L6-v2` (å½“å‰é»˜è®¤)
- æˆ–è€…ä½¿ç”¨ GPU: è®¾ç½® `device: "cuda"`

### Q5: PDF æå–ä¹±ç 

**åŸå› **: PDF åŒ…å«æ‰«æå›¾ç‰‡

**è§£å†³**:
- ç­‰å¾… Phase 4 (OCR æ”¯æŒ)
- æˆ–æ‰‹åŠ¨è½¬æ¢ä¸ºæ–‡æœ¬åæ‘„å–

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### é»˜è®¤é…ç½®æ€§èƒ½

**æ–‡æ¡£å¤„ç†é€Ÿåº¦**:
- PDF (10é¡µ): ~2-3 ç§’
- TXT (100KB): ~0.5 ç§’
- DOCX (20é¡µ): ~3-4 ç§’

**å‘é‡åŒ–é€Ÿåº¦** (CPU):
- 100 ä¸ªå—: ~10-15 ç§’
- 1000 ä¸ªå—: ~2-3 åˆ†é’Ÿ

**æŸ¥è¯¢é€Ÿåº¦**:
- å‘é‡æ£€ç´¢: ~0.1-0.5 ç§’
- LLM ç”Ÿæˆ: ~2-5 ç§’
- æ€»å»¶è¿Ÿ: ~3-6 ç§’

### GPU åŠ é€Ÿ

è®¾ç½® `device: "cuda"` å:
- å‘é‡åŒ–é€Ÿåº¦: 3-5x æå‡
- æŸ¥è¯¢é€Ÿåº¦: 2x æå‡

---

## ğŸ¯ ä¸‹ä¸€æ­¥

å®ŒæˆåŸºæœ¬ RAG æµ‹è¯•å:

1. **Phase 2**: æ·»åŠ é‡æ’åº â†’ æå‡å‡†ç¡®ç‡
2. **Phase 3**: é›†æˆé¢†åŸŸå·¥å…· â†’ æ»¡è¶³é¡¹ç›®è¦æ±‚
3. **Phase 4**: æ·»åŠ å¤šæ¨¡æ€ â†’ æ”¯æŒå›¾ç‰‡/å¤æ‚ PDF
4. **Phase 5**: å·¥ä½œæµå¼•æ“ â†’ å¤„ç†å¤æ‚æŸ¥è¯¢

è¯¦è§ `IMPLEMENTATION_PROGRESS.md`

---

## ğŸ’¡ é«˜çº§æŠ€å·§

### æŠ€å·§ 1: æ··åˆæ£€ç´¢ï¼ˆRAG + Web Searchï¼‰

```python
# å…ˆæŸ¥ RAG
rag_result = await rag_agent.query("é—®é¢˜")

# å¦‚æœ RAG æ²¡æ‰¾åˆ°ï¼Œç”¨ Web Search
if rag_result['retrieved_chunks'] == 0:
    web_result = await research_agent.research("é—®é¢˜")
    # åˆå¹¶ç»“æœ
```

### æŠ€å·§ 2: å¢é‡æ›´æ–°

```python
# åªæ·»åŠ æ–°æ–‡æ¡£ï¼Œä¸æ¸…ç©ºæ—§æ–‡æ¡£
await rag_agent.ingest_document("new_doc.pdf")
```

### æŠ€å·§ 3: å…ƒæ•°æ®è¿‡æ»¤

```python
# æœªæ¥æ”¯æŒï¼šæŒ‰å…ƒæ•°æ®è¿‡æ»¤
# results = vector_store.similarity_search(
#     query="é—®é¢˜",
#     where={"source": {"$contains": "2024"}}
# )
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

- **Chroma æ–‡æ¡£**: https://docs.trychroma.com/
- **LlamaIndex æ–‡æ¡£**: https://docs.llamaindex.ai/
- **Sentence Transformers**: https://www.sbert.net/
- **é¡¹ç›®è¿›åº¦**: `IMPLEMENTATION_PROGRESS.md`
- **å¼€å‘æŒ‡å—**: `CLAUDE.md`

---

ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼æœ‰é—®é¢˜æ¬¢è¿æŸ¥çœ‹ `IMPLEMENTATION_PROGRESS.md` æˆ–æ Issueã€‚
